{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Caption Preprocessing","metadata":{}},{"cell_type":"code","source":"import string\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2021-12-05T18:20:40.956541Z","iopub.execute_input":"2021-12-05T18:20:40.956890Z","iopub.status.idle":"2021-12-05T18:20:40.985822Z","shell.execute_reply.started":"2021-12-05T18:20:40.956786Z","shell.execute_reply":"2021-12-05T18:20:40.985164Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#load all captions in memory\ndef load_captions(cap_data):\n    #create a dict containing id corresponding to caption\n    mapping = dict()\n    for line in cap_data.split('\\n')[1:]:\n        token = line.split(',')\n        #ignore very short caption\n        if len(line) < 2:\n            continue\n        image_id = token[0].split('.')[0] #image id\n        image_cap = token[1] #image caption\n        if image_id not in mapping:\n            mapping[image_id] = []\n        mapping[image_id].append(image_cap)\n    return mapping\n \ncfile_path = '../input/flickr8k/captions.txt'\ncap_data = open(cfile_path, 'r', encoding='utf-8').read()\nmapping = load_captions(cap_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T18:20:45.362389Z","iopub.execute_input":"2021-12-05T18:20:45.363191Z","iopub.status.idle":"2021-12-05T18:20:45.512904Z","shell.execute_reply.started":"2021-12-05T18:20:45.363144Z","shell.execute_reply":"2021-12-05T18:20:45.512166Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Preforming text preprocessing\ndef cap_clean(mapping):\n    for key, cap_list in mapping.items():\n        for i in range(len(cap_list)):\n            #load each caption\n            caption = cap_list[i]\n            #convert to lower case\n            caption = caption.lower()\n            #remove punctuation marks\n            caption = [word for word in caption if word not in string.punctuation]\n            caption = ''.join(caption)\n            #remove unwanted words\n            caption = caption.split(' ')\n            caption = [word for word in caption if len(word)>1 and word.isalpha()]\n            caption = ' '.join(caption)\n            #save\n            cap_list[i] = caption\n\ncap_clean(mapping)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T18:20:46.940675Z","iopub.execute_input":"2021-12-05T18:20:46.941628Z","iopub.status.idle":"2021-12-05T18:20:47.356575Z","shell.execute_reply.started":"2021-12-05T18:20:46.941584Z","shell.execute_reply":"2021-12-05T18:20:47.355836Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#generating vocabulary\n#library of unique words in our caption data\ndef get_vocab(mapping):\n    words = set()\n    for key in mapping.keys():\n        for line in mapping[key]:\n            words.update(line.split())\n    return words\n\nvocab = get_vocab(mapping)\nprint(len(vocab))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T18:20:47.645496Z","iopub.execute_input":"2021-12-05T18:20:47.646126Z","iopub.status.idle":"2021-12-05T18:20:47.702348Z","shell.execute_reply.started":"2021-12-05T18:20:47.646088Z","shell.execute_reply":"2021-12-05T18:20:47.701625Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#load images to memory\n#train and test split\nimport glob\nimport os\nimg_path = '../input/flickr8k/Images/'\n#creating a list of image file names\nimg_list = glob.glob(img_path + '*jpg')\n\n#train and test split\n#train_images = 7000\n#test_images = 1091\ntrain_path = 'trainImg.txt'\nwith open(train_path, 'a') as f:\n    for path in img_list[:7000]:\n        f.write(path+'\\n')\ntest_path = 'testImg.txt'\nwith open(test_path, 'a') as f:\n    for path in img_list[7000:]:\n        f.write(path+'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T18:20:48.906389Z","iopub.execute_input":"2021-12-05T18:20:48.907370Z","iopub.status.idle":"2021-12-05T18:20:49.129890Z","shell.execute_reply.started":"2021-12-05T18:20:48.907311Z","shell.execute_reply":"2021-12-05T18:20:49.129004Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_img = open(train_path, 'r').read()\ntest_img = open(test_path, 'r').read()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T18:20:49.488423Z","iopub.execute_input":"2021-12-05T18:20:49.489464Z","iopub.status.idle":"2021-12-05T18:20:49.494526Z","shell.execute_reply.started":"2021-12-05T18:20:49.489416Z","shell.execute_reply":"2021-12-05T18:20:49.493803Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_img = train_img.split('\\n')\ntrain_img = train_img[:7000]\ntrain_img[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img = test_img.split('\\n')\ntest_img = test_img[:1091]\ntest_img[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fetch all image ids in training data\ndef get_dataset(data_path):\n    data_id = []\n    for path in data_path:\n        path = path.split('/')\n        id = path[-1].split('.')[0]\n        data_id.append(id)\n    return data_id\n\ntrain_id = get_dataset(train_img)\nprint(len(train_id))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding <start> and <end> tags in each caption in given dataset(train or test)\ndef load_tagged_cap(mapping, dataset):\n    tagged_map = dict()\n    for key, cap_list in mapping.items():\n        if key in dataset:\n            if key not in tagged_map:\n                tagged_map[key] = []\n            for line in cap_list:\n                tagged_line = 'startseq ' + line + ' endseq'\n                tagged_map[key].append(tagged_line)\n    return tagged_map\n\ntrain_cap = load_tagged_cap(mapping, train_id)\nprint(train_cap['3226254560_2f8ac147ea'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Processing","metadata":{}},{"cell_type":"code","source":"#import all required libraries\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n#load Inception model for transfer learning\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.list_physical_devices()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add to memory\ndef process_img(img_path):\n    with tpu_strategy.scope():\n        #load with size 299 x 299 as inception_v2 accept that\n        img = load_img(img_path, target_size=(299, 299, 3))\n        img_arr = img_to_array(img)\n        #expand by a dimension and scale pixels from -1 to 1 range\n        img_arr = np.expand_dims(img_arr, axis=0)\n        img_arr = preprocess_input(img_arr)\n        return img_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n#get features using inception model\nbase_model = InceptionV3(weights='imagenet', include_top=False,\n                         input_shape=(299, 299, 3), pooling=max)\nmodel = Model(base_model.input, base_model.layers[-1].output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_feature(img_arr):\n    with tf.device('/GPU:0'):\n        feature_vec = model.predict(img_arr)\n        feature_vec = np.reshape(feature_vec, (feature_vec.shape[0],\n                        feature_vec.shape[1]*feature_vec.shape[2]*feature_vec.shape[3]))\n        return feature_vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_feature_map(data_img_path, data_img_id):\n    #creating a feature map \n    with tf.device('/GPU:0'):\n        encoded_feature_map = dict()\n        with tqdm(total=len(train_id)) as pbar:\n            for img_path, img_id in zip(data_img_path, data_img_id):\n                #preprocess image and encode the feature vector\n                img_arr = process_img(img_path)\n                img_vec = encode_feature(img_arr)\n                encoded_feature_map[img_id] = img_vec\n                pbar.update(1)\n        return encoded_feature_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device('/GPU:0'):\n    encoded_feature_map = get_feature_map(train_img, train_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save feature map\nfile = 'feature_map.pickle'\noutfile = open(file, 'wb')\npickle.dump(encoded_feature_map, outfile)\noutfile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"train_caption = []\nfor key, cap_list in train_cap.items():\n    for cap in cap_list:\n        train_caption.append(cap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#max length of caption available in train data\nmax_len = max(len(cap.split()) for cap in train_caption)\nmax_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cleaning vocabulary\nthreshold = 10 #add only words having frequency > 10\nword_count = dict()\nfor cap in train_caption:\n    for word in cap.split(' '):\n        word_count[word] = word_count.get(word, 0) + 1\n\nvocab = [word for word in word_count if word_count[word] >= threshold]\n\n#mapping each word in vocabulary with integer\nwordtoint = dict()\ninttoword = dict()\n\nfor word, i in zip(vocab, range(1,len(vocab)+1)):\n    wordtoint[word] = i\n    inttoword[i] = word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outfile_tokenize1 = open('wordtoint.pickle', 'wb')\noutfile_tokenize2 = open('inttoword.pickle', 'wb')\npickle.dump(wordtoint, outfile_tokenize1)\npickle.dump(inttoword, outfile_tokenize2)\noutfile_tokenize1.close()\noutfile_tokenize2.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inttoword[4], wordtoint['the']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data generation\nAdding data into *input -> output* form\n, here we have two inputs namely: \nX1 -> Input image features &\nX2 -> Input seq\nand One Output\nY1 -> Output seq\nUsing LSTM\nI am going feed in sequence such that LSTM network has to predict every other word in \nsequence as output given previous word as input in sequence.","metadata":{}},{"cell_type":"code","source":"infile = open('feature_map.pickle', 'rb')\nfeature_map = pickle.load(infile)\ninfile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical, plot_model\nX1, X2, y = [], [], []\n\nfor key, cap_list in train_cap.items():\n    img_feature = feature_map[key]\n    for cap in cap_list:\n        seq = [wordtoint[word] for word in cap.split() if word in wordtoint]\n        for i in range(1, len(seq)):\n            in_sq, out_sq = seq[:i], seq[i]\n            in_sq = pad_sequences([in_sq], maxlen=max_len)[0]\n            out_sq = to_categorical([out_sq], num_classes=len(vocab)+1)[0]\n            X1.append(img_feature)\n            X2.append(in_sq)\n            y.append(out_sq)\n\nX1 = np.asarray(X1)\nX2 = np.asarray(X2)\ny = np.asarray(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a Embedding layer using GLOVE\nglove is global vectors for word representation, version I am using here consist of 6 billion words from english and each word is have 200 dimensional vector for it and it is pre trained word to vector model thus I just need to find one from kaggle and add it to my dataset.\nI am going to create a Embedding layer with it such that I can get *vocab_size x 200d* Embedding layer in model.","metadata":{}},{"cell_type":"code","source":"embedding_map = {}\nglove_path = '../input/glove6b/glove.6B.200d.txt'\nglove = open(glove_path, 'r', encoding='utf-8').read()\nfor line in glove.split(\"\\n\"):\n    val = line.split(\" \")\n    word = val[0]\n    vec = np.asarray(val[1: ], dtype = 'float32')\n    embedding_map[word] = vec\n\nemb_dim = 200\nemb_matrix = np.zeros((len(vocab)), emb_dim)\nfor word, i in wordtoint.items():\n    emb_vec = embedding_map.get(word)\n    if emb_vec is not None:\n        emb_matrix[i] = emb_vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model design","metadata":{}},{"cell_type":"code","source":"from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\nfrom keras.layers import concatenate, BatchNormalization, Input\nfrom keras.layers.merge import add","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature extraction\nip1 = Input(shape = (2048, ))\nfe1 = Dropout(0.5)(ip1)\nfe2 = Dense(256, activation = 'relu')(fe1)\n\n#LSTM layers \nip2 = Input(shape = (max_length, ))\nse1 = Embedding(len(vocab), emb_dim, mask_zero = True)(ip2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\n#add function\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation = 'relu')(decoder1)\noutputs = Dense(vocab_size, activation = 'softmax')(decoder2)\nmodel = Model(inputs = [ip1, ip2], outputs = outputs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[2].set_weights([emb_matrix])\nmodel.layers[2].trainable = False\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\nmodel.fit([X1, X2], y, epochs = 50, batch_size = 256)\nmodel.save('model1.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting output","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model = load_model('trained_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedy_search(pic, trained_model):\n    start = 'startseq'\n    for i in range(max_length):\n        seq = [wordtoix[word] for word in start.split() if word in wordtoix]\n        seq = pad_sequences([seq], maxlen = max_length)\n        yhat = trained_model.predict([pic, seq])\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        start += ' ' + word\n        if word == 'endseq':\n            break\n    final = start.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for one image\ndef input_img(img_path):\n    img_arr = process_img(img_path)\n    img_vec = encode_feature(img_arr)\n    return img_vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#on testing data\ntest_id = get_dataset(test_img)\ntest_cap = load_tagged_cap(mapping, test_id)\nwith tf.device('/GPU:0'):\n    test_features = get_feature_map(test_img, test_id)\nfor key, features in test_features.items():\n    pred_cap = greedy_search(features, trained_model)","metadata":{},"execution_count":null,"outputs":[]}]}