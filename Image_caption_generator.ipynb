{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b741720c",
   "metadata": {},
   "source": [
    "# cleansing captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289dc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project path\n",
    "prpath = r'C:\\Users\\nbxyz\\Desktop\\image-caption-generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60761af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25acc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file containing captions\n",
    "def load_data(fname):\n",
    "    #open file and read\n",
    "    file = open(fname, 'r')\n",
    "    read_txt = file.read()\n",
    "    #close file\n",
    "    file.close()\n",
    "    return read_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ec5e4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get descriptions for images in dataset\n",
    "def extract_desc(file):\n",
    "    #store in dict\n",
    "    desc_map = {}\n",
    "    \n",
    "    #process each caption\n",
    "    for cap in file.split('\\n'):\n",
    "        #get tokens\n",
    "        tokens = cap.split()\n",
    "        #ignore very small captions\n",
    "        if len(cap) < 2:\n",
    "            continue\n",
    "        # mapping token0 : img_id and token1: img_desc\n",
    "        img_id, img_desc = tokens[0], tokens[1:]\n",
    "        #remove filename from img_id\n",
    "        img_id = img_id.split('.')[0]\n",
    "        #convert img_desc tokens to string format\n",
    "        img_desc = ' '.join(img_desc)\n",
    "        # create the list if needed\n",
    "        if img_id not in desc_map:\n",
    "            desc_map[img_id] = []\n",
    "        # store description\n",
    "        desc_map[img_id].append(img_desc)\n",
    "    return desc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2480e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning descriptions extracted\n",
    "def clean_desc(descriptions):\n",
    "    # translation table for removal of punctuations\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # remove punctuation from each token\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e8068c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting image descriptions to vocabulary\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    img_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [img_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return img_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b508fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61719925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start caption cleaning process\n",
    "\n",
    "# get data\n",
    "caption_data_path = prpath + '\\img_caption_generator\\dataset\\captions.txt'\n",
    "cap_data = load_data(caption_data_path)\n",
    "# extract descriptions\n",
    "img_desc = extract_desc(cap_data)\n",
    "# clean descriptions\n",
    "clean_desc(img_desc)\n",
    "# get vocabulary\n",
    "vocab = to_vocabulary(img_desc)\n",
    "# save\n",
    "save_descriptions(img_desc, 'descriptions.txt')\n",
    "\n",
    "print('Loaded: %d ' % len(img_desc))\n",
    "print('Vocabulary Size: %d' % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2fa4e6",
   "metadata": {},
   "source": [
    "# generate feature vector for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0918f169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc684c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pickle import dump\n",
    "# Import pre-trained Xception model for extracting features\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "# for progress bar \n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9566baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Xception model to get feature vector from images\n",
    "def extract_features(directory):\n",
    "        model = Xception( include_top=False, pooling='avg' )\n",
    "        features = {}\n",
    "        for img in tqdm(os.listdir(directory)):\n",
    "            filename = directory + \"/\" + img\n",
    "            image = Image.open(filename)\n",
    "            image = image.resize((299,299))\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = image/127.5\n",
    "            image = image - 1.0\n",
    "            feature = model.predict(image)\n",
    "            features[img] = feature\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    # extract features from images\n",
    "    image_path = prpath + '/dataset/Images'\n",
    "    features = extract_features(image_path)\n",
    "    # dump pickel file containing features\n",
    "    dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1957aaf",
   "metadata": {},
   "source": [
    "# load dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a59ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8302ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b719c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b561c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4745a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = []\n",
    "    # filter features\n",
    "    for k in dataset:\n",
    "        key = str(k)+'.jpg'\n",
    "        features.append(all_features[key])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a81b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddb52c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    dump(tokenizer, open(prpath + '\\\\tokenizer.pkl', 'wb'))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599d830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def get_max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548e4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fb0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d04fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        counter = 0\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[counter][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093882a7",
   "metadata": {},
   "source": [
    "# training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f0c4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prpath + '\\dataset\\\\train.txt'\n",
    "test_data = prpath + '\\dataset\\\\test.txt'\n",
    "desc_path = prpath + '\\\\descriptions.txt'\n",
    "features_path = prpath + '\\\\features.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f00bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 7000\n",
      "Descriptions: train=7000\n",
      "Photos: train=7000\n",
      "Vocabulary Size: 8166\n",
      "Description Length: 33\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 33)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 33, 256)      2090496     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 33, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8166)         2098662     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,304,806\n",
      "Trainable params: 5,304,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_10636/2924307180.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd0256119754b23b07eae9f82585480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_10636/2924307180.py:28: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 938s 134ms/step - loss: 4.6909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 924s 132ms/step - loss: 3.9570\n",
      "7000/7000 [==============================] - 897s 128ms/step - loss: 3.7067\n",
      "7000/7000 [==============================] - 963s 138ms/step - loss: 3.5474\n",
      "7000/7000 [==============================] - 1014s 145ms/step - loss: 3.4341\n",
      "7000/7000 [==============================] - 945s 135ms/step - loss: 3.3484\n",
      "7000/7000 [==============================] - 880s 126ms/step - loss: 3.2844\n",
      "7000/7000 [==============================] - 878s 125ms/step - loss: 3.2354\n",
      "7000/7000 [==============================] - 956s 137ms/step - loss: 3.1887\n",
      "6687/7000 [===========================>..] - ETA: 41s - loss: 3.1502"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "train = load_set(train_data)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions(desc_path, train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features(features_path, train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_len = get_max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_len)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_len)\n",
    "    # train the model, run epochs manually and save after each epoch\n",
    "    epochs = 10\n",
    "    steps = len(train_descriptions)\n",
    "    for i in tqdm(range(epochs)):\n",
    "        # create the data generator\n",
    "        generator = data_generator(train_descriptions, train_features, tokenizer, max_len, vocab_size)\n",
    "        # fit for one epoch\n",
    "        model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "        # save model\n",
    "        model.save(prpath + '\\model\\model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "788a3e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 7000\n",
      "Descriptions: train=7000\n",
      "Photos: train=7000\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "# load training dataset\n",
    "train = load_set(train_data)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions(desc_path, train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features(features_path, train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b1d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "from numpy import argmax\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b9a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085cc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1413e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1732371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = []\n",
    "    # filter features\n",
    "    for k in dataset:\n",
    "        key = str(k)+'.jpg'\n",
    "        features.append(all_features[key])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3659763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c01a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbd02454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def get_max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02c5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68dc1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5517fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    counter = 0\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[counter], max_length)\n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "        counter += 1\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e07108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 7000\n",
      "Descriptions: train=7000\n",
      "Vocabulary Size: 8166\n",
      "Description Length: 33\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer on train set\n",
    "\n",
    "# load training dataset\n",
    "train = load_set(train_data)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_len = get_max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b10ca",
   "metadata": {},
   "source": [
    "# test set evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test = load_set(test_data)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions(desc_path, test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features(features_path, test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = r'C:\\Users\\nbxyz\\Desktop\\image-caption-generate\\model\\model_9.h5'\n",
    "model = load_model(filename)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    # evaluate model\n",
    "    evaluate_model(model, test_descriptions, test_features, tokenizer, max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
